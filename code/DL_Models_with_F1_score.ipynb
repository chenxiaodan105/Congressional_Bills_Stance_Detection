{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models with F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# data preprocessing\n",
    "import data_preprocessing\n",
    "\n",
    "# nlp packages\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# machine learning packages\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517cd36787a64217885d65282c2a7528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47428), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa7c17b2588409f8400c60a2a785566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47428), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = data_preprocessing.read_data('./data/data.csv')\n",
    "train_corpus, test_corpus, train_labels, test_labels = data_preprocessing.split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing for deep learning models\n",
    "\n",
    "**keep speeches whose length are between a minimum occurence and a maximum occurence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove speeches whose length is shorter than 5 or longer than 200 for training data\n",
    "def get_fixed_length_range_data(corpus,labels,min_len,max_len):\n",
    "    '''\n",
    "    remove speeches whose length is shorter than min_len or longer than max_len \n",
    "    input: speeches\n",
    "    output: target corpus, target labels\n",
    "    '''\n",
    "    new_corpus = []\n",
    "    new_labels = []\n",
    "    for i in range(len(corpus)):\n",
    "        if len(corpus[i].split()) > min_len and len(corpus[i].split()) <= max_len :\n",
    "            new_corpus.append(corpus[i])\n",
    "            new_labels.append(labels[i])\n",
    "        else:\n",
    "            continue\n",
    "    return new_corpus, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_corpus, new_train_labels = get_fixed_length_range_data(train_corpus,train_labels,5,200)\n",
    "new_test_corpus, new_test_labels = get_fixed_length_range_data(test_corpus,test_labels,5,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel '-1' to '0' for train_labels for later deep learning model\n",
    "for i in range(len(new_train_labels)):\n",
    "    if new_train_labels[i]==-1:\n",
    "        new_train_labels[i]=0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel '-1' to '0' for test_labels for later deep learning model\n",
    "for j in range(len(new_test_labels)):\n",
    "    if new_test_labels[j]==-1:\n",
    "        new_test_labels[j]=0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**keep sppeches only with tokens with a minimum occurence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_tokens(min_occurence,max_occurence):\n",
    "    '''\n",
    "    get the vocab of the whole corpus\n",
    "    '''\n",
    "    vocab = Counter()\n",
    "    corpus = new_train_corpus + new_test_corpus\n",
    "    for speech in corpus:\n",
    "        tokens = speech.split()\n",
    "        vocab.update(tokens)\n",
    "    # keep tokens with a min occurence\n",
    "    common_tokens = [k for k,c in vocab.items() if c > min_occurence and c < max_occurence]\n",
    "    \n",
    "    # new vocab\n",
    "    new_dic = {}\n",
    "    for k,c in vocab.items(): \n",
    "        if c > min_occurence and c < max_occurence:\n",
    "            new_dic[k]=c\n",
    "        \n",
    "    return common_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus,min_occurence,max_occurence):\n",
    "    '''\n",
    "    ensure all speeches in a corpus only keep tokens with a min occurence\n",
    "    input: corpus, common tokens with a min occurence in the whole corpus\n",
    "    output: new target corpus\n",
    "    '''\n",
    "    common_tokens = get_common_tokens(min_occurence,max_occurence)\n",
    "    new_corpus = []\n",
    "    for i in tqdm(range(len(corpus))):\n",
    "        tokens = corpus[i].split()\n",
    "        tokens = [w for w in tokens if w in common_tokens]\n",
    "        new_speech = ' '.join(tokens)\n",
    "        new_corpus.append(new_speech)\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93523214f14f48d797a6f884f5277050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18673), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_train_corpus2 = clean_corpus(new_train_corpus,1000,80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11ee205b9e04d7dabe0b56f451f6612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7896), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_test_corpus2 = clean_corpus(new_test_corpus,1000,80000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pretrained GloVe word embedding + LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "corpus = new_train_corpus2 + new_test_corpus2\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "#tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i > num_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(new_train_corpus2)\n",
    "test_sequences = tokenizer.texts_to_sequences(new_test_corpus2)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=200)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./GloVe/glove.6B.100d.txt') \n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 200, 100)          41000     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 121,501\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 41,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model = models.Sequential()\n",
    "glove_model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=200, trainable=False))\n",
    "glove_model.add(Dropout(0.2))\n",
    "glove_model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.2))\n",
    "glove_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#With the set_weights method we load the pre-trained embeddings in the Embedding layer (here layer 0). \n",
    "#By setting the trainable attribute to False, we make sure not to change the pre-trained embeddings.\n",
    "glove_model.layers[0].set_weights([embedding_matrix])\n",
    "glove_model.layers[0].trainable = False\n",
    "glove_model.summary()\n",
    "#plot_model(glove_model,to_file='glove_lstm.png',show_shapes=True)\n",
    "glove_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a66d63ac8>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.fit(train_padded, \n",
    "                np.array(new_train_labels), \n",
    "                epochs=10,\n",
    "                verbose=False,\n",
    "                validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96      5765\n",
      "           1       0.99      0.75      0.86      2131\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      7896\n",
      "   macro avg       0.95      0.88      0.91      7896\n",
      "weighted avg       0.94      0.93      0.93      7896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get F1 score\n",
    "predictions = glove_model.predict(test_padded)\n",
    "new_predictions = []\n",
    "for i in predictions.reshape(1,-1)[0]:\n",
    "    if i > 0.5:\n",
    "        new_predictions.append(1)\n",
    "    else:\n",
    "        new_predictions.append(0)\n",
    "print(classification_report(np.array(new_test_labels),new_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pretrained GloVe word embedding + CNN + LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_cnn_lstm_model(vocab_size):\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocab_size, 100, input_length=200))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(32, 3, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=2))\n",
    "    model_conv.add(LSTM(100))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.layers[0].set_weights([embedding_matrix])\n",
    "    model_conv.layers[0].trainable = False\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    model_conv.summary()\n",
    "    #plot_model(model_conv, to_file='cnn_lstm.png', show_shapes=True)\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 200, 100)          41000     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 198, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 99, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 103,933\n",
      "Trainable params: 62,933\n",
      "Non-trainable params: 41,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "glove_model_CNN_LSTM = define_cnn_lstm_model(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5d47e0b8>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "glove_model_CNN_LSTM.fit(train_padded, \n",
    "                        np.array(new_train_labels), \n",
    "                        epochs=10,\n",
    "                        verbose=False,\n",
    "                        validation_split=0.3)\n",
    "\n",
    "# save the model\n",
    "#glove_model_CNN_LSTM.save('glove_model_CNN_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95      5765\n",
      "           1       0.96      0.78      0.86      2131\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      7896\n",
      "   macro avg       0.94      0.88      0.91      7896\n",
      "weighted avg       0.93      0.93      0.93      7896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get F1 score\n",
    "predictions = glove_model_CNN_LSTM.predict(test_padded)\n",
    "new_predictions = []\n",
    "for i in predictions.reshape(1,-1)[0]:\n",
    "    if i > 0.5:\n",
    "        new_predictions.append(1)\n",
    "    else:\n",
    "        new_predictions.append(0)\n",
    "print(classification_report(np.array(new_test_labels),new_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
